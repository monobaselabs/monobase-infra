# Custom Prometheus Alert Rules
# Application-specific alerts for HapiHub, MongoDB, MinIO, Velero

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: hapihub-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
    # HapiHub Application Alerts
    - name: hapihub
      interval: 30s
      rules:
        - alert: HapiHubDown
          expr: up{job="hapihub"} == 0
          for: 5m
          labels:
            severity: critical
            component: hapihub
          annotations:
            summary: "HapiHub is down"
            description: "HapiHub has been down for more than 5 minutes"

        - alert: HapiHubHighErrorRate
          expr: rate(http_requests_total{job="hapihub",status=~"5.."}[5m]) > 0.05
          for: 5m
          labels:
            severity: warning
            component: hapihub
          annotations:
            summary: "HapiHub high error rate"
            description: "HapiHub error rate is {{ $value | humanizePercentage }}"

        - alert: HapiHubHighLatency
          expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="hapihub"}[5m])) > 2
          for: 10m
          labels:
            severity: warning
            component: hapihub
          annotations:
            summary: "HapiHub high latency"
            description: "HapiHub p95 latency is {{ $value }}s"

        - alert: HapiHubHighMemoryUsage
          expr: container_memory_usage_bytes{pod=~"hapihub-.*"} / container_spec_memory_limit_bytes{pod=~"hapihub-.*"} > 0.9
          for: 10m
          labels:
            severity: warning
            component: hapihub
          annotations:
            summary: "HapiHub high memory usage"
            description: "HapiHub memory usage is {{ $value | humanizePercentage }}"

    # MongoDB Alerts
    - name: mongodb
      interval: 30s
      rules:
        - alert: MongoDBDown
          expr: mongodb_up == 0
          for: 5m
          labels:
            severity: critical
            component: mongodb
          annotations:
            summary: "MongoDB is down"
            description: "MongoDB has been down for more than 5 minutes"

        - alert: MongoDBReplicationLag
          expr: mongodb_replset_member_replication_lag > 10
          for: 5m
          labels:
            severity: critical
            component: mongodb
          annotations:
            summary: "MongoDB replication lag detected"
            description: "Replication lag is {{ $value }} seconds"

        - alert: MongoDBHighConnections
          expr: mongodb_connections{state="current"} / mongodb_connections{state="available"} > 0.8
          for: 10m
          labels:
            severity: warning
            component: mongodb
          annotations:
            summary: "MongoDB high connection usage"
            description: "Connection usage is {{ $value | humanizePercentage }}"

        - alert: MongoDBHighMemory
          expr: mongodb_memory{type="resident"} / 1024 / 1024 / 1024 > 5
          for: 10m
          labels:
            severity: warning
            component: mongodb
          annotations:
            summary: "MongoDB high memory usage"
            description: "MongoDB using {{ $value }}GB memory"

        - alert: MongoDBSlowQueries
          expr: rate(mongodb_op_latencies_latency_total{type="command"}[5m]) > 1000
          for: 10m
          labels:
            severity: warning
            component: mongodb
          annotations:
            summary: "MongoDB slow queries detected"
            description: "Average query latency is {{ $value }}ms"

    # MinIO Alerts
    - name: minio
      interval: 30s
      rules:
        - alert: MinIODown
          expr: up{job="minio"} == 0
          for: 5m
          labels:
            severity: critical
            component: minio
          annotations:
            summary: "MinIO is down"
            description: "MinIO has been down for more than 5 minutes"

        - alert: MinIODiskOffline
          expr: minio_cluster_disk_offline_total > 0
          for: 5m
          labels:
            severity: critical
            component: minio
          annotations:
            summary: "MinIO disk offline"
            description: "{{ $value }} MinIO disks are offline"

        - alert: MinIOHighStorage
          expr: minio_cluster_capacity_usable_free_bytes / minio_cluster_capacity_usable_total_bytes < 0.2
          for: 10m
          labels:
            severity: warning
            component: minio
          annotations:
            summary: "MinIO storage running low"
            description: "Only {{ $value | humanizePercentage }} storage remaining"

    # Velero Backup Alerts
    - name: velero
      interval: 60s
      rules:
        - alert: VeleroBackupFailed
          expr: velero_backup_failure_total > 0
          for: 5m
          labels:
            severity: critical
            component: velero
          annotations:
            summary: "Velero backup failed"
            description: "{{ $value }} backup failures detected"

        - alert: VeleroBackupPartialFailure
          expr: velero_backup_partial_failure_total > 0
          for: 5m
          labels:
            severity: warning
            component: velero
          annotations:
            summary: "Velero backup partially failed"
            description: "{{ $value }} partial backup failures"

        - alert: VeleroNoRecentBackup
          expr: time() - velero_backup_last_successful_timestamp > 86400
          for: 1h
          labels:
            severity: warning
            component: velero
          annotations:
            summary: "No recent Velero backup"
            description: "Last successful backup was {{ $value | humanizeDuration }} ago"

    # Storage Alerts
    - name: storage
      interval: 60s
      rules:
        - alert: PersistentVolumeFillingUp
          expr: |
            (
              kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes
            ) < 0.2
          for: 10m
          labels:
            severity: warning
            component: storage
          annotations:
            summary: "PersistentVolume filling up"
            description: "PVC {{ $labels.persistentvolumeclaim }} has only {{ $value | humanizePercentage }} free space"

        - alert: LonghornVolumeUnhealthy
          expr: longhorn_volume_robustness != 0
          for: 5m
          labels:
            severity: critical
            component: longhorn
          annotations:
            summary: "Longhorn volume unhealthy"
            description: "Volume {{ $labels.volume }} is in degraded state"

# Grafana configuration continued
grafana:
  # Additional configuration
  downloadDashboardsImage:
    repository: curlimages/curl
    tag: latest
    pullPolicy: IfNotPresent

  # Security context
  securityContext:
    runAsNonRoot: true
    runAsUser: 472
    fsGroup: 472
    seccompProfile:
      type: RuntimeDefault

  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL

  # Sidecar for dashboard auto-reload
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard

# Alertmanager
alertmanager:
  enabled: true

  alertmanagerSpec:
    # Storage
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

    # Resources
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        cpu: 200m
        memory: 256Mi

    # Replicas
    replicas: 2

    # Security context
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000
      seccompProfile:
        type: RuntimeDefault

# Configuration managed via ConfigMap or External Secrets

# Exporters
nodeExporter:
  enabled: true

kubeStateMetrics:
  enabled: true

# RBAC
rbac:
  create: true

# Installation:
# helm install monitoring prometheus-community/kube-prometheus-stack \
#   -n monitoring --create-namespace -f helm-values.yaml
